{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe0bf24",
   "metadata": {},
   "source": [
    "We first construct the simulated bibliographies so that they have the descriptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ef781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biography: The person is detail-oriented and likes public speaking. They served in the military for four years. Previously, they worked in finance and budgeting and focused on data and analytics. They earned a BA and supported operations, and has international experience in Europe. In their free time, the person enjoys painting and museums and volunteers with environmental groups.\n",
      "T: [1 0 0 1 0] Y: [-0.62633984 -3.61121937  5.7760097   4.95511758 -1.71149162]\n"
     ]
    }
   ],
   "source": [
    "# simulation on bilography data\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def simulate_biographies(\n",
    "    n=2000,\n",
    "    k=6,\n",
    "    tau=1.0,\n",
    "    seed=7\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    U = rng.normal(size=(n, k))\n",
    "\n",
    "    # Treatment depends on U (confounding) but not deterministic\n",
    "    beta = rng.normal(size=k)\n",
    "    p = sigmoid(U @ beta / 1.5)  # soften to keep overlap\n",
    "    T = rng.binomial(1, p, size=n)\n",
    "\n",
    "    # Outcome depends on T and U\n",
    "    gamma = rng.normal(size=k)\n",
    "    Y = tau * T + U @ gamma + rng.normal(scale=1.0, size=n)\n",
    "\n",
    "    # Convert U into coarse “traits” that show up as text confounders\n",
    "    # (topic-ish phrases)\n",
    "    def trait_phrases(u_row):\n",
    "        phrases = []\n",
    "        if u_row[0] > 0: phrases.append(\"worked in finance and budgeting\")\n",
    "        else:            phrases.append(\"worked in education and mentoring\")\n",
    "\n",
    "        if u_row[1] > 0: phrases.append(\"focused on data and analytics\")\n",
    "        else:            phrases.append(\"focused on community outreach\")\n",
    "\n",
    "        if u_row[2] > 0: phrases.append(\"earned an MBA and led teams\")\n",
    "        else:            phrases.append(\"earned a BA and supported operations\")\n",
    "\n",
    "        if u_row[3] > 0: phrases.append(\"enjoys long-distance running\")\n",
    "        else:            phrases.append(\"enjoys painting and museums\")\n",
    "\n",
    "        if u_row[4] > 0: phrases.append(\"has international experience in Asia\")\n",
    "        else:            phrases.append(\"has international experience in Europe\")\n",
    "\n",
    "        if u_row[5] > 0: phrases.append(\"volunteers with youth programs\")\n",
    "        else:            phrases.append(\"volunteers with environmental groups\")\n",
    "        return phrases\n",
    "\n",
    "    # Treatment feature: a specific textual attribute\n",
    "    treat_phrase = \"served in the military for four years\"\n",
    "\n",
    "    texts = []\n",
    "    for i in range(n):\n",
    "        base = trait_phrases(U[i])\n",
    "        # Add some random fluff to prevent trivial keyword-only embeddings\n",
    "        fluff = [\"values teamwork\", \"is detail-oriented\", \"likes public speaking\", \"enjoys reading history\"]\n",
    "        rng.shuffle(fluff)\n",
    "        parts = [\n",
    "            \"Biography:\",\n",
    "            f\"The person {fluff[0]} and {fluff[1]}.\",\n",
    "            \"Previously, they \" + base[0] + \" and \" + base[1] + \".\",\n",
    "            \"They \" + base[2] + \", and \" + base[4] + \".\",\n",
    "            \"In their free time, the person \" + base[3] + \" and \" + base[5] + \".\"\n",
    "        ]\n",
    "        if T[i] == 1:\n",
    "            # Treatment is a feature *inside the text*\n",
    "            parts.insert(2, f\"They {treat_phrase}.\")\n",
    "        texts.append(\" \".join(parts))\n",
    "\n",
    "    return texts, T.astype(int), Y.astype(float), U, p\n",
    "\n",
    "# Quick smoke test\n",
    "texts, T, Y, U, p_true = simulate_biographies(n=5)\n",
    "print(texts[0])\n",
    "print(\"T:\", T[:5], \"Y:\", Y[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd68090a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pangu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_texts(\n",
    "    texts,\n",
    "    model_name=\"distilbert-base-uncased\",\n",
    "    pooling=\"cls\",   # \"cls\" | \"last\" | \"mean\"\n",
    "    max_length=256,\n",
    "    batch_size=32,\n",
    "    device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns: embeddings (n, d) as a numpy array.\n",
    "\n",
    "    pooling:\n",
    "      - \"cls\": encoder models with [CLS] token representation\n",
    "      - \"last\": decoder-only (or generic) last token representation\n",
    "      - \"mean\": mean-pool across tokens (mask-aware)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_name) # from a pre-trained model for tokenization\n",
    "    if tok.pad_token is None: # for decode only models (e.g., GPT-2, as they don't have a pad token by default)\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    model = AutoModel.from_pretrained(model_name, output_hidden_states=True) # load pre-trained model for inference\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tok(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        out = model(**enc) # forward propagation using the pre-trained model and generate the last hidden state\n",
    "        # out.last_hidden_state: (B, L, H)\n",
    "        h = out.last_hidden_state\n",
    "        attn = enc[\"attention_mask\"]  # (B, L) # as enc outputs two tensors: last_hidden_state and attention_mask\n",
    "\n",
    "        if pooling == \"cls\": #transfer (B, L, H) to (B, H) (a vector for a specific text/batch)\n",
    "            # For BERT-like models, token 0 is typically [CLS]\n",
    "            vec = h[:, 0, :]\n",
    "        elif pooling == \"last\":\n",
    "            # Take the last non-pad token for each row\n",
    "            lengths = attn.sum(dim=1)  # (B,)\n",
    "            idx = (lengths - 1).clamp(min=0)\n",
    "            vec = h[torch.arange(h.size(0)), idx, :]\n",
    "        elif pooling == \"mean\":\n",
    "            # Mask-aware mean pooling\n",
    "            mask = attn.unsqueeze(-1)  # (B, L, 1)\n",
    "            summed = (h * mask).sum(dim=1)\n",
    "            denom = mask.sum(dim=1).clamp(min=1)\n",
    "            vec = summed / denom\n",
    "        else:\n",
    "            raise ValueError(\"pooling must be one of: cls, last, mean\")\n",
    "\n",
    "        all_vecs.append(vec.detach().cpu())\n",
    "\n",
    "    emb = torch.cat(all_vecs, dim=0).numpy() # so the output will be a \n",
    "    return emb\n",
    "\n",
    "# Example usage:\n",
    "# BERT-ish\n",
    "R = embed_texts(texts, model_name=\"distilbert-base-uncased\", pooling=\"cls\")\n",
    "# Decoder-only small model (proxy for Llama-style last-token pooling)\n",
    "# R = embed_texts(texts, model_name=\"distilgpt2\", pooling=\"last\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb2e8f",
   "metadata": {},
   "source": [
    "Extract the eigens for each texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea35ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# as R is now a (B,H) matrix, we need to use a function g(R) to get Z (the deconfounder) \n",
    "# why we need de-confounding: the positivity assumption in causal inference \\pi(T = 1 \\mid R)\n",
    "def make_deconfounder(R, dz=50):\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    Rz = scaler.fit_transform(R)\n",
    "    pca = PCA(n_components=dz, random_state=0)\n",
    "    Z = pca.fit_transform(Rz)\n",
    "    return Z, scaler, pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da0ab053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "import numpy as np\n",
    "\n",
    "# establish the doubly robust ML model \n",
    "# naive estimators: propensity model \\pi_A = P(A\\mid g(R)); outcome model: \\mu_1(R), \\mu_0(R) = Y \\mid A = a, Z = g(R)\n",
    "def dml_ate_crossfit(Z, T, Y, n_splits=5, seed=0):\n",
    "    \"\"\"\n",
    "    Cross-fitted EIF ATE estimator.\n",
    "    Returns: ate, se, psi (influence values)\n",
    "    \"\"\"\n",
    "    Z = np.asarray(Z)\n",
    "    T = np.asarray(T).astype(int)\n",
    "    Y = np.asarray(Y).astype(float)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    psi_all = np.zeros_like(Y, dtype=float)\n",
    "\n",
    "    eps = 1e-3  # clipping for stability\n",
    "\n",
    "    for train_idx, test_idx in kf.split(Z):\n",
    "        Ztr, Zte = Z[train_idx], Z[test_idx]\n",
    "        Ttr, Tte = T[train_idx], T[test_idx]\n",
    "        Ytr, Yte = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        # Propensity model\n",
    "        e_model = LogisticRegression(max_iter=2000)\n",
    "        e_model.fit(Ztr, Ttr)\n",
    "        e = e_model.predict_proba(Zte)[:, 1]\n",
    "        e = np.clip(e, eps, 1 - eps)\n",
    "\n",
    "        # Outcome models: separate regressions by treatment\n",
    "        mu1_model = Ridge(alpha=1.0)\n",
    "        mu0_model = Ridge(alpha=1.0)\n",
    "\n",
    "        mu1_model.fit(Ztr[Ttr == 1], Ytr[Ttr == 1])\n",
    "        mu0_model.fit(Ztr[Ttr == 0], Ytr[Ttr == 0])\n",
    "\n",
    "        mu1 = mu1_model.predict(Zte)\n",
    "        mu0 = mu0_model.predict(Zte)\n",
    "\n",
    "        # Efficient influence function\n",
    "        psi = (mu1 - mu0) + (Tte * (Yte - mu1) / e) - ((1 - Tte) * (Yte - mu0) / (1 - e))\n",
    "        psi_all[test_idx] = psi\n",
    "\n",
    "    ate = psi_all.mean()\n",
    "    se = psi_all.std(ddof=1) / np.sqrt(len(Y))\n",
    "    return ate, se, psi_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "837173c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True tau = 1.000\n",
      "DML ATE  = 5.954 (SE 1.828)\n",
      "95% CI   = [2.371, 9.537]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'texts': ['Biography: The person is detail-oriented and values teamwork. They served in the military for four years. Previously, they worked in finance and budgeting and focused on data and analytics. They earned a BA and supported operations, and has international experience in Europe. In their free time, the person enjoys painting and museums and volunteers with environmental groups.',\n",
       "  'Biography: The person values teamwork and likes public speaking. They served in the military for four years. Previously, they worked in finance and budgeting and focused on data and analytics. They earned a BA and supported operations, and has international experience in Asia. In their free time, the person enjoys painting and museums and volunteers with youth programs.',\n",
       "  'Biography: The person is detail-oriented and enjoys reading history. They served in the military for four years. Previously, they worked in finance and budgeting and focused on community outreach. They earned a BA and supported operations, and has international experience in Europe. In their free time, the person enjoys long-distance running and volunteers with environmental groups.',\n",
       "  'Biography: The person likes public speaking and enjoys reading history. Previously, they worked in education and mentoring and focused on community outreach. They earned a BA and supported operations, and has international experience in Europe. In their free time, the person enjoys painting and museums and volunteers with youth programs.',\n",
       "  'Biography: The person values teamwork and is detail-oriented. They served in the military for four years. Previously, they worked in finance and budgeting and focused on community outreach. They earned a BA and supported operations, and has international experience in Europe. In their free time, the person enjoys painting and museums and volunteers with youth programs.',\n",
       "  'Biography: The person likes public speaking and enjoys reading history. Previously, they worked in education and mentoring and focused on community outreach. They earned a BA and supported operations, and has international experience in Asia. In their free time, the person enjoys painting and museums and volunteers with environmental groups.',\n",
       "  'Biography: The person likes public speaking and enjoys reading history. They served in the military for four years. Previously, they worked in education and mentoring and focused on data and analytics. They earned a BA and supported operations, and has international experience in Asia. In their free time, the person enjoys painting and museums and volunteers with youth programs.',\n",
       "  'Biography: The person enjoys reading history and is detail-oriented. They served in the military for four years. Previously, they worked in education and mentoring and focused on data and analytics. They earned an MBA and led teams, and has international experience in Asia. In their free time, the person enjoys painting and museums and volunteers with youth programs.',\n",
       "  'Biography: The person enjoys reading history and likes public speaking. Previously, they worked in education and mentoring and focused on data and analytics. They earned an MBA and led teams, and has international experience in Asia. In their free time, the person enjoys painting and museums and volunteers with youth programs.',\n",
       "  'Biography: The person enjoys reading history and is detail-oriented. They served in the military for four years. Previously, they worked in education and mentoring and focused on data and analytics. They earned a BA and supported operations, and has international experience in Asia. In their free time, the person enjoys long-distance running and volunteers with environmental groups.'],\n",
       " 'T': array([1, 1, 1, 0, 1, 0, 1, 1, 0, 1]),\n",
       " 'Y': array([ 0.44957537, -0.77462942,  2.89213072,  0.5460774 ,  0.80140285,\n",
       "        -0.9631369 ,  1.75998679, -1.07529472, -2.77644111,  2.10112423]),\n",
       " 'R': array([[-0.12095645, -0.03379363, -0.04698665, ..., -0.14453007,\n",
       "          0.39801708,  0.10378117],\n",
       "        [-0.12765746, -0.03449184, -0.03709103, ..., -0.07512613,\n",
       "          0.40363058,  0.06557146],\n",
       "        [-0.15863207, -0.04364786, -0.11627191, ..., -0.06399605,\n",
       "          0.35277554,  0.15712222],\n",
       "        ...,\n",
       "        [-0.17602834, -0.11126715, -0.01019625, ..., -0.14069925,\n",
       "          0.4196279 ,  0.08964305],\n",
       "        [-0.16869578, -0.08696912,  0.0122451 , ..., -0.16171986,\n",
       "          0.45105538,  0.10500772],\n",
       "        [-0.18700215, -0.0522083 , -0.04597057, ..., -0.12538323,\n",
       "          0.34253576,  0.11122841]], dtype=float32),\n",
       " 'Z': array([[  8.384111  ,  -9.651998  , -11.022011  ],\n",
       "        [  5.2169204 ,  -1.3328714 , -12.71222   ],\n",
       "        [ 12.729206  , -17.320898  ,  12.454983  ],\n",
       "        [-28.524635  ,  -4.926933  ,   6.3709273 ],\n",
       "        [ -7.9354506 , -11.39303   , -21.60846   ],\n",
       "        [-22.312141  ,  -4.688341  ,  13.70689   ],\n",
       "        [  7.0627174 ,   7.770176  ,   0.22129223],\n",
       "        [  7.754335  ,  17.847425  ,  -7.3522344 ],\n",
       "        [ -7.160224  ,  25.28663   ,   3.8351781 ],\n",
       "        [ 24.785164  ,  -1.590149  ,  16.105673  ]], dtype=float32),\n",
       " 'ate': 5.954002298919145,\n",
       " 'se': 1.8278900200991048}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_replication(\n",
    "    n=1000,\n",
    "    model_name=\"distilgpt2\",  # try \"distilbert-base-uncased\" too\n",
    "    pooling=\"last\",           # \"cls\" for BERT, \"last\" for GPT2-like\n",
    "    dz=50,\n",
    "    tau_true=1.0,\n",
    "    seed=7\n",
    "):\n",
    "    texts, T, Y, U, p_true = simulate_biographies(n=n, tau=tau_true, seed=seed)\n",
    "\n",
    "    # 1) Embed texts\n",
    "    R = embed_texts(texts, model_name=model_name, pooling=pooling, max_length=256, batch_size=32)\n",
    "\n",
    "    # 2) Deconfounder Z = g(R)\n",
    "    Z, scaler, pca = make_deconfounder(R, dz=dz)\n",
    "\n",
    "    # 3) DML ATE\n",
    "    ate, se, psi = dml_ate_crossfit(Z, T, Y, n_splits=5, seed=seed)\n",
    "\n",
    "    print(f\"True tau = {tau_true:.3f}\")\n",
    "    print(f\"DML ATE  = {ate:.3f} (SE {se:.3f})\")\n",
    "    print(f\"95% CI   = [{ate - 1.96*se:.3f}, {ate + 1.96*se:.3f}]\")\n",
    "    return {\"texts\": texts, \"T\": T, \"Y\": Y, \"R\": R, \"Z\": Z, \"ate\": ate, \"se\": se}\n",
    "\n",
    "# Example:\n",
    "results  = run_replication(n=10, model_name=\"distilbert-base-uncased\", pooling=\"cls\", dz=3, tau_true=1.0)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b830611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrations on multi-modal for causal representation learning can be added here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5da9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance loss (fine-tuning for the encoding function g(.)) (CORAL method)\n",
    "# Z = encoder(X) -> (B, d)\n",
    "# e_hat = propensity_head(Z) -> (B,)\n",
    "# y0_hat, y1_hat = outcome_heads(Z) -> (B,), (B,)\n",
    "\n",
    "loss_t = bce(e_hat, T.float())\n",
    "\n",
    "# factual outcome loss: only supervise observed potential outcome\n",
    "y_hat = T * y1_hat + (1 - T) * y0_hat\n",
    "loss_y = mse(y_hat, Y)\n",
    "\n",
    "# CORAL balance\n",
    "Z1 = Z[T==1]; Z0 = Z[T==0]\n",
    "mu1, mu0 = Z1.mean(0), Z0.mean(0)\n",
    "C1 = (Z1 - mu1).T @ (Z1 - mu1) / (len(Z1)-1 + 1e-6)\n",
    "C0 = (Z0 - mu0).T @ (Z0 - mu0) / (len(Z0)-1 + 1e-6)\n",
    "loss_bal = ((mu1-mu0)**2).mean() + ((C1-C0)**2).mean()\n",
    "\n",
    "loss = loss_y + lam_t*loss_t + lam_bal*loss_bal\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
