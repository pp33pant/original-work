{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28842d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "try:\n",
    "    import hdbscan\n",
    "    HAS_HDBSCAN = True\n",
    "except Exception:\n",
    "    HAS_HDBSCAN = False\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "\n",
    "def batched(iterable, batch_size: int):\n",
    "    n = len(iterable)\n",
    "    for i in range(0, n, batch_size):\n",
    "        yield i, iterable[i:i + batch_size]\n",
    "\n",
    "def safe_text(x: Any) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    s = str(x).strip()\n",
    "    return s\n",
    "\n",
    "def l2norm(x: np.ndarray) -> np.ndarray:\n",
    "    return normalize(x, norm=\"l2\", axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# c-TF-IDF style keyword labeling\n",
    "# -----------------------------\n",
    "def c_tf_idf_keywords(\n",
    "    texts: List[str],\n",
    "    labels: np.ndarray,\n",
    "    top_n: int = 10,\n",
    "    min_df: int = 3,\n",
    "    max_df: float = 0.90,\n",
    "    stop_words: str = \"english\",\n",
    ") -> Dict[int, List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    Compute cluster keywords with a c-TF-IDF-like approach:\n",
    "    - Aggregate documents per cluster\n",
    "    - CountVectorizer\n",
    "    - cTFIDF = (TF normalized within cluster) * IDF across clusters\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"text\": texts, \"label\": labels})\n",
    "    # filter noise label -1\n",
    "    df = df[df[\"label\"] >= 0].copy()\n",
    "    if df.empty:\n",
    "        return {}\n",
    "\n",
    "    grouped = df.groupby(\"label\")[\"text\"].apply(lambda xs: \" \".join(xs)).reset_index()\n",
    "    cluster_docs = grouped[\"text\"].tolist()\n",
    "    cluster_ids = grouped[\"label\"].tolist()\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "        stop_words=stop_words,\n",
    "        min_df=min_df,\n",
    "        max_df=max_df,\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "    X = vectorizer.fit_transform(cluster_docs)  # shape: (n_clusters, vocab)\n",
    "    vocab = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "    # TF: normalize term counts by total terms in cluster\n",
    "    tf = X.astype(np.float64)\n",
    "    tf = tf.multiply(1.0 / np.clip(tf.sum(axis=1), 1, None))\n",
    "\n",
    "    # IDF across clusters\n",
    "    df_term = np.asarray((X > 0).sum(axis=0)).ravel()\n",
    "    n_clusters = X.shape[0]\n",
    "    idf = np.log((n_clusters + 1) / (df_term + 1)) + 1.0  # smooth\n",
    "\n",
    "    ctfidf = tf.multiply(idf)  # (n_clusters, vocab)\n",
    "\n",
    "    out: Dict[int, List[Tuple[str, float]]] = {}\n",
    "    for row_idx, cid in enumerate(cluster_ids):\n",
    "        row = np.asarray(ctfidf[row_idx].todense()).ravel()\n",
    "        if row.max() <= 0:\n",
    "            out[cid] = []\n",
    "            continue\n",
    "        top_idx = np.argsort(-row)[:top_n]\n",
    "        out[cid] = [(vocab[i], float(row[i])) for i in top_idx if row[i] > 0]\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Data structures\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class CanonicalTheme:\n",
    "    theme_id: int\n",
    "    size: int\n",
    "    centroid: np.ndarray  # shape (dim,)\n",
    "    keywords: List[Tuple[str, float]]\n",
    "    label: str\n",
    "\n",
    "@dataclass\n",
    "class CanonicalThemeSet:\n",
    "    sector: str\n",
    "    model_name: str\n",
    "    embedding_dim: int\n",
    "    themes: List[CanonicalTheme]\n",
    "\n",
    "    def to_dataframe(self) -> pd.DataFrame:\n",
    "        rows = []\n",
    "        for t in self.themes:\n",
    "            rows.append({\n",
    "                \"sector\": self.sector,\n",
    "                \"theme_id\": t.theme_id,\n",
    "                \"size\": t.size,\n",
    "                \"label\": t.label,\n",
    "                \"keywords\": [k for k, _ in t.keywords],\n",
    "                \"keyword_scores\": [s for _, s in t.keywords],\n",
    "                \"centroid\": t.centroid.astype(np.float32),\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Canonical theme builder\n",
    "# -----------------------------\n",
    "class CanonicalThemeBuilder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"BAAI/bge-large-en-v1.5\",\n",
    "        device: Optional[str] = None,\n",
    "        batch_size: int = 64,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> np.ndarray:\n",
    "        embs = []\n",
    "        for _, batch in tqdm(list(batched(texts, self.batch_size)), desc=\"Embedding\"):\n",
    "            e = self.model.encode(\n",
    "                batch,\n",
    "                show_progress_bar=False,\n",
    "                normalize_embeddings=True,  # important for cosine similarity\n",
    "            )\n",
    "            embs.append(e)\n",
    "        return np.vstack(embs)\n",
    "\n",
    "    def cluster_embeddings(\n",
    "        self,\n",
    "        embeddings: np.ndarray,\n",
    "        method: str = \"hdbscan\",\n",
    "        min_cluster_size: int = 40,\n",
    "        min_samples: int = 10,\n",
    "        kmeans_k: int = 50,\n",
    "        random_state: int = 42,\n",
    "    ) -> np.ndarray:\n",
    "        if method == \"hdbscan\":\n",
    "            if not HAS_HDBSCAN:\n",
    "                raise RuntimeError(\"hdbscan not installed; either install hdbscan or use method='kmeans'.\")\n",
    "            clusterer = hdbscan.HDBSCAN(\n",
    "                min_cluster_size=min_cluster_size,\n",
    "                min_samples=min_samples,\n",
    "                metric=\"euclidean\",\n",
    "                prediction_data=False,\n",
    "            )\n",
    "            labels = clusterer.fit_predict(embeddings)\n",
    "            return labels\n",
    "        elif method == \"kmeans\":\n",
    "            km = KMeans(n_clusters=kmeans_k, random_state=random_state, n_init=\"auto\")\n",
    "            return km.fit_predict(embeddings)\n",
    "        else:\n",
    "            raise ValueError(\"method must be 'hdbscan' or 'kmeans'\")\n",
    "\n",
    "    def build(\n",
    "        self,\n",
    "        df_chunks: pd.DataFrame, # a data frame for chunks \n",
    "        sector: str,\n",
    "        text_col: str = \"text\",\n",
    "        sector_col: str = \"sector\",\n",
    "        method: str = \"hdbscan\",\n",
    "        min_cluster_size: int = 40,\n",
    "        min_samples: int = 10,\n",
    "        kmeans_k: int = 50,\n",
    "        top_keywords: int = 10,\n",
    "        min_df: int = 3,\n",
    "        max_df: float = 0.90,\n",
    "    ) -> Tuple[CanonicalThemeSet, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          - CanonicalThemeSet: sector-level canonical themes\n",
    "          - df_out: original chunks with assigned theme_id (cluster label), including noise (-1)\n",
    "        \"\"\"\n",
    "        df = df_chunks.copy()\n",
    "        df = df[df[sector_col] == sector].copy()\n",
    "        df[text_col] = df[text_col].map(safe_text)\n",
    "        df = df[df[text_col].str.len() > 0].copy()\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        texts = df[text_col].tolist()\n",
    "        embs = self.embed_texts(texts) # transfer chunks into embeddings \n",
    "\n",
    "        labels = self.cluster_embeddings(\n",
    "            embs,\n",
    "            method=method,\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            kmeans_k=kmeans_k # in this code we use k_means to generate the topic clusters (themes); we can also use chatbot to achieve this \n",
    "        )\n",
    "\n",
    "        # keywords / labels (c-TF-IDF) (need to use RAG to make the labels on the themes understandable)\n",
    "        kw = c_tf_idf_keywords(\n",
    "            texts=texts,\n",
    "            labels=labels,\n",
    "            top_n=top_keywords,\n",
    "            min_df=min_df,\n",
    "            max_df=max_df,\n",
    "            stop_words=\"english\",\n",
    "        )\n",
    "\n",
    "        themes: List[CanonicalTheme] = []\n",
    "        for theme_id in sorted(set(labels)):\n",
    "            if theme_id < 0:\n",
    "                continue\n",
    "            idx = np.where(labels == theme_id)[0]\n",
    "            centroid = embs[idx].mean(axis=0) # get the centriod of each theme cluster (the average embedding vector of each theme)\n",
    "            centroid = centroid / (np.linalg.norm(centroid) + 1e-12)\n",
    "\n",
    "            keywords = kw.get(theme_id, [])\n",
    "            label_str = \", \".join([k for k, _ in keywords[:5]]) if keywords else f\"theme_{theme_id}\"\n",
    "            themes.append(\n",
    "                CanonicalTheme(\n",
    "                    theme_id=int(theme_id),\n",
    "                    size=int(len(idx)),\n",
    "                    centroid=centroid.astype(np.float32),\n",
    "                    keywords=keywords,\n",
    "                    label=label_str,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        theme_set = CanonicalThemeSet(\n",
    "            sector=sector,\n",
    "            model_name=self.model_name,\n",
    "            embedding_dim=int(embs.shape[1]),\n",
    "            themes=themes\n",
    "        )\n",
    "\n",
    "        df_out = df_chunks.copy()\n",
    "        # attach labels for sector rows only; others remain NaN\n",
    "        df_out[\"theme_id\"] = np.nan\n",
    "        df_out.loc[df_out[sector_col] == sector, \"theme_id\"] = labels\n",
    "\n",
    "        return theme_set, df_out\n",
    "\n",
    "\n",
    "def save_theme_set(theme_set: CanonicalThemeSet, path_parquet: str, path_npz: str) -> None:\n",
    "    \"\"\"\n",
    "    Save:\n",
    "      - Parquet for metadata/keywords/labels\n",
    "      - NPZ for centroid matrix\n",
    "    \"\"\"\n",
    "    df = theme_set.to_dataframe()\n",
    "    df.to_parquet(path_parquet, index=False)\n",
    "\n",
    "    # pack centroids\n",
    "    theme_ids = [t.theme_id for t in theme_set.themes]\n",
    "    centroids = np.vstack([t.centroid for t in theme_set.themes]).astype(np.float32)\n",
    "    np.savez_compressed(path_npz, sector=theme_set.sector, theme_ids=theme_ids, centroids=centroids, model=theme_set.model_name)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from canonical_themes import CanonicalThemeBuilder, save_theme_set\n",
    "\n",
    "df = pd.read_parquet(\"chunks.parquet\")  # all chunks\n",
    "builder = CanonicalThemeBuilder(model_name=\"BAAI/bge-large-en-v1.5\", batch_size=64)\n",
    "\n",
    "theme_set, df_labeled = builder.build(\n",
    "    df_chunks=df,\n",
    "    sector=\"beauty/care\",\n",
    "    method=\"hdbscan\",          # hdbscan or kmeans\n",
    "    min_cluster_size=50,\n",
    "    min_samples=10,\n",
    "    top_keywords=12,\n",
    ")\n",
    "\n",
    "save_theme_set(theme_set, \"beauty_care_themes.parquet\", \"beauty_care_centroids.npz\")\n",
    "df_labeled.to_parquet(\"chunks_with_theme_id.parquet\", index=False)\n",
    "   # save the theme set as two files: one is parquet file to store the metadata/keywords/labels; another is npz file to store the centroid matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d57acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# referral \n",
    "from __future__ import annotations\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_centroids(npz_path: str) -> Tuple[np.ndarray, np.ndarray, str]:\n",
    "    z = np.load(npz_path, allow_pickle=True)\n",
    "    theme_ids = np.array(z[\"theme_ids\"], dtype=int)\n",
    "    centroids = np.array(z[\"centroids\"], dtype=np.float32)\n",
    "    model_name = str(z[\"model\"])\n",
    "    return theme_ids, centroids, model_name\n",
    "\n",
    "def embed_texts(model: SentenceTransformer, texts: list[str], batch_size: int = 64) -> np.ndarray:\n",
    "    embs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding new chunks\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        e = model.encode(batch, show_progress_bar=False, normalize_embeddings=True)\n",
    "        embs.append(e)\n",
    "    return np.vstack(embs)\n",
    "\n",
    "def assign_themes_by_centroid(\n",
    "    df_new_chunks: pd.DataFrame, # so we have new chunks to be assigned to existing themes\n",
    "    centroids_npz: str,\n",
    "    text_col: str = \"text\",\n",
    "    batch_size: int = 64,\n",
    "    sim_threshold: float = 0.35, # set the threshold, if the chunks are not matched here, they may need future \n",
    "    device: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each chunk, compute cosine similarity to all theme centroids.\n",
    "    If max_sim < threshold => theme_id = -1 (new candidate)\n",
    "    \"\"\"\n",
    "    theme_ids, centroids, model_name = load_centroids(centroids_npz)\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    df = df_new_chunks.copy()\n",
    "    df[text_col] = df[text_col].fillna(\"\").astype(str)\n",
    "    texts = df[text_col].tolist()\n",
    "\n",
    "    embs = embed_texts(model, texts, batch_size=batch_size)  # normalized\n",
    "    sims = cosine_similarity(embs, centroids)  # shape (n_chunks, n_themes) (calculate the cosine similarity between each chunk and each theme centroid)\n",
    "\n",
    "    best_idx = sims.argmax(axis=1)\n",
    "    best_sim = sims.max(axis=1)\n",
    "    assigned_theme = theme_ids[best_idx]\n",
    "\n",
    "    df[\"theme_id\"] = assigned_theme\n",
    "    df[\"theme_sim\"] = best_sim\n",
    "    df.loc[df[\"theme_sim\"] < sim_threshold, \"theme_id\"] = -1\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4901f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the new themes (for chunks that are not assigned to any existing themes)\n",
    "from __future__ import annotations\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "try:\n",
    "    import hdbscan\n",
    "    HAS_HDBSCAN = True\n",
    "except Exception:\n",
    "    HAS_HDBSCAN = False\n",
    "\n",
    "from canonical_themes import c_tf_idf_keywords\n",
    "\n",
    "def propose_new_themes(\n",
    "    df_unassigned: pd.DataFrame,\n",
    "    emb_col: str = \"embedding\", # the column name for embeddings\n",
    "    text_col: str = \"text\",\n",
    "    method: str = \"hdbscan\",\n",
    "    min_cluster_size: int = 30,\n",
    "    min_samples: int = 10,\n",
    "    kmeans_k: int = 20,\n",
    "    top_keywords: int = 10,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Input should contain only chunks with theme_id == -1.\n",
    "    Requires embeddings.\n",
    "    \"\"\"\n",
    "    df = df_unassigned.copy()\n",
    "    texts = df[text_col].fillna(\"\").astype(str).tolist()\n",
    "    embs = np.vstack(df[emb_col].values).astype(np.float32)\n",
    "\n",
    "    if method == \"hdbscan\":\n",
    "        if not HAS_HDBSCAN:\n",
    "            raise RuntimeError(\"hdbscan not installed; use method='kmeans' or install hdbscan\")\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "        labels = clusterer.fit_predict(embs)\n",
    "    else:\n",
    "        km = KMeans(n_clusters=kmeans_k, random_state=42, n_init=\"auto\")\n",
    "        labels = km.fit_predict(embs)\n",
    "\n",
    "    kw = c_tf_idf_keywords(texts=texts, labels=labels, top_n=top_keywords)\n",
    "\n",
    "    df[\"candidate_cluster\"] = labels\n",
    "    # attach label string\n",
    "    label_map = {}\n",
    "    for cid, kws in kw.items():\n",
    "        label_map[cid] = \", \".join([k for k, _ in kws[:5]]) if kws else f\"candidate_{cid}\"\n",
    "    df[\"candidate_label\"] = df[\"candidate_cluster\"].map(label_map)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640e6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using RAG to make themes understandable/readable from human's perspective\n",
    "# establish the FAISS index for retrieval \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "def embed_texts(model: SentenceTransformer, texts: list[str], batch_size: int = 64) -> np.ndarray:\n",
    "    embs = model.encode(texts, batch_size=batch_size, normalize_embeddings=True, show_progress_bar=True)\n",
    "    return np.asarray(embs, dtype=np.float32)\n",
    "\n",
    "def build_faiss_index(chunks_df: pd.DataFrame, model_name: str = \"BAAI/bge-large-en-v1.5\"):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    texts = chunks_df[\"text\"].fillna(\"\").astype(str).tolist()\n",
    "    embs = embed_texts(model, texts)\n",
    "\n",
    "    dim = embs.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)  # inner product == cosine (if normalized)\n",
    "    index.add(embs)\n",
    "    return model, index, embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the chunks that are most relevant to each theme (retrieve process)\n",
    "def retrieve_chunks(\n",
    "    query: str,\n",
    "    model: SentenceTransformer,\n",
    "    index: faiss.IndexFlatIP,\n",
    "    chunks_df: pd.DataFrame,\n",
    "    top_k: int = 30,\n",
    "    source_type: str | None = None,     # \"transcript\"/\"comment\"/None\n",
    "    video_id: str | None = None,\n",
    "    min_like: int | None = None\n",
    ") -> pd.DataFrame:\n",
    "    df = chunks_df\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "\n",
    "    if source_type is not None:\n",
    "        mask &= (df[\"source_type\"] == source_type)\n",
    "    if video_id is not None:\n",
    "        mask &= (df[\"video_id\"] == video_id)\n",
    "    if min_like is not None and \"like_count\" in df.columns:\n",
    "        mask &= (df[\"like_count\"].fillna(0) >= min_like)\n",
    "\n",
    "    df_sub = df[mask].copy()\n",
    "    if df_sub.empty:\n",
    "        return df_sub\n",
    "\n",
    "    # IMPORTANT: FAISS index is built on full df order.\n",
    "    # For filtering, simplest is to search wider then filter down.\n",
    "    q_emb = model.encode([query], normalize_embeddings=True)\n",
    "    q_emb = np.asarray(q_emb, dtype=np.float32)\n",
    "\n",
    "    D, I = index.search(q_emb, top_k * 10)\n",
    "    idx = I[0].tolist()\n",
    "\n",
    "    df_hits = chunks_df.iloc[idx].copy()\n",
    "    if source_type is not None:\n",
    "        df_hits = df_hits[df_hits[\"source_type\"] == source_type]\n",
    "    if video_id is not None:\n",
    "        df_hits = df_hits[df_hits[\"video_id\"] == video_id]\n",
    "    if min_like is not None and \"like_count\" in df_hits.columns:\n",
    "        df_hits = df_hits[df_hits[\"like_count\"].fillna(0) >= min_like]\n",
    "\n",
    "    return df_hits.head(top_k).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622f23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the retrieved chunks, we can use LLM to generate the theme labels\n",
    "import json\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "def _compact_evidence_rows(df_hits: pd.DataFrame, max_items: int = 12, max_chars: int = 260) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    for _, r in df_hits.head(max_items).iterrows():\n",
    "        text = str(r[\"text\"])\n",
    "        text = text.replace(\"\\n\", \" \").strip()\n",
    "        if len(text) > max_chars:\n",
    "            text = text[:max_chars].rstrip() + \"…\"\n",
    "\n",
    "        rows.append({\n",
    "            \"chunk_id\": r.get(\"chunk_id\"),\n",
    "            \"video_id\": r.get(\"video_id\"),\n",
    "            \"source_type\": r.get(\"source_type\"),\n",
    "            \"start_sec\": r.get(\"start_sec\", None),\n",
    "            \"end_sec\": r.get(\"end_sec\", None),\n",
    "            \"comment_id\": r.get(\"comment_id\", None),\n",
    "            \"thread_id\": r.get(\"thread_id\", None),\n",
    "            \"like_count\": int(r.get(\"like_count\", 0) or 0),\n",
    "            \"snippet\": text\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def name_theme_with_rag(\n",
    "    theme_id: int,\n",
    "    theme_query: str,  # e.g., generated from keywords or \"Summarize the core theme...\"\n",
    "    df_evidence: pd.DataFrame,\n",
    "    llm_call,          # function(prompt)->str  returns JSON string\n",
    ") -> Dict[str, Any]:\n",
    "    evidence_rows = _compact_evidence_rows(df_evidence)\n",
    "\n",
    "    schema_hint = {\n",
    "        \"theme_id\": theme_id,\n",
    "        \"theme_name\": \"Short readable name (3-8 words)\",\n",
    "        \"one_line_definition\": \"One sentence definition\",\n",
    "        \"keywords\": [\"keyword1\", \"keyword2\", \"...\"],\n",
    "        \"evidence_chunks\": [\n",
    "            {\"chunk_id\": \"…\", \"why_representative\": \"…\"}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are naming a canonical theme for a sector-level taxonomy.\n",
    "You MUST ground the name in the provided evidence snippets.\n",
    "Return STRICT JSON only (no markdown, no commentary).\n",
    "\n",
    "Theme query/context:\n",
    "{theme_query}\n",
    "\n",
    "Evidence snippets (each has chunk_id and location metadata):\n",
    "{json.dumps(evidence_rows, ensure_ascii=False, indent=2)}\n",
    "\n",
    "Output JSON schema example:\n",
    "{json.dumps(schema_hint, ensure_ascii=False, indent=2)}\n",
    "\n",
    "Rules:\n",
    "- theme_name must be concise and readable.\n",
    "- evidence_chunks must reference ONLY chunk_id from the evidence list above.\n",
    "- Provide 5-10 keywords that reflect the theme.\n",
    "\"\"\"\n",
    "\n",
    "    raw = llm_call(prompt)\n",
    "    try:\n",
    "        out = json.loads(raw)\n",
    "    except Exception:\n",
    "        # very common: model returns extra text; attempt to extract JSON\n",
    "        start = raw.find(\"{\")\n",
    "        end = raw.rfind(\"}\")\n",
    "        out = json.loads(raw[start:end+1])\n",
    "\n",
    "    return out\n",
    "\n",
    "# pip install openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def llm_call_openai(prompt: str) -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",  # 示例：换成你可用的模型\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You output strict JSON only.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the names \n",
    "def build_theme_cards(\n",
    "    theme_ids: list[int],\n",
    "    theme_queries: dict[int, str],   # theme_id -> query string\n",
    "    model: SentenceTransformer,\n",
    "    index: faiss.IndexFlatIP,\n",
    "    chunks_df: pd.DataFrame,\n",
    "    llm_call,\n",
    "    top_k_evidence: int = 30\n",
    ") -> pd.DataFrame:\n",
    "    cards = []\n",
    "    for tid in theme_ids:\n",
    "        q = theme_queries.get(tid, f\"Identify the central theme and key aspects for theme {tid}.\")\n",
    "        # RAG evidence: retrieve from whole sector corpus\n",
    "        df_hits = retrieve_chunks(\n",
    "            query=q,\n",
    "            model=model,\n",
    "            index=index,\n",
    "            chunks_df=chunks_df,\n",
    "            top_k=top_k_evidence,\n",
    "            source_type=None\n",
    "        )\n",
    "\n",
    "        card = name_theme_with_rag(\n",
    "            theme_id=tid,\n",
    "            theme_query=q,\n",
    "            df_evidence=df_hits,\n",
    "            llm_call=llm_call\n",
    "        )\n",
    "        cards.append(card)\n",
    "\n",
    "    return pd.DataFrame(cards)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
